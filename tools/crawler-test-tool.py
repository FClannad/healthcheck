#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥åº·ä½“æ£€ç³»ç»Ÿ - çˆ¬è™«åŠŸèƒ½æµ‹è¯•å·¥å…·
ç‹¬ç«‹æµ‹è¯•çˆ¬è™«æœåŠ¡ï¼Œæ— éœ€å¯åŠ¨å®Œæ•´çš„Spring Bootåº”ç”¨
"""

import requests
import json
import time
import sys
from datetime import datetime

class CrawlerTestTool:
    def __init__(self, base_url="http://localhost:9090"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'User-Agent': 'Health-Check-System-Crawler-Test/1.0'
        })

    def test_backend_health(self):
        """æµ‹è¯•åç«¯å¥åº·çŠ¶æ€"""
        try:
            print("ğŸ” æµ‹è¯•åç«¯è¿æ¥...")
            response = self.session.get(f"{self.base_url}/actuator/health", timeout=10)
            if response.status_code == 200:
                data = response.json()
                print(f"âœ… åç«¯æœåŠ¡æ­£å¸¸: {data.get('status', 'UP')}")
                return True
            else:
                print(f"âŒ åç«¯æœåŠ¡å¼‚å¸¸: HTTP {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            print(f"âŒ åç«¯è¿æ¥å¤±è´¥: {e}")
            return False

    def test_crawler_service(self, keyword="äººå·¥æ™ºèƒ½", count=5):
        """æµ‹è¯•çˆ¬è™«æœåŠ¡"""
        try:
            print(f"ğŸ•·ï¸ æµ‹è¯•çˆ¬è™«æœåŠ¡ - å…³é”®è¯: {keyword}, æ•°é‡: {count}")
            
            payload = {
                "keyword": keyword,
                "count": count
            }
            
            response = self.session.post(
                f"{self.base_url}/medical-literature/crawl",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                print(f"âœ… çˆ¬è™«æµ‹è¯•æˆåŠŸ!")
                print(f"   - å“åº”çŠ¶æ€: {data.get('code', 'unknown')}")
                print(f"   - å“åº”æ¶ˆæ¯: {data.get('msg', 'no message')}")
                print(f"   - çˆ¬å–æ•°é‡: {data.get('data', 0)}")
                return True
            else:
                print(f"âŒ çˆ¬è™«æµ‹è¯•å¤±è´¥: HTTP {response.status_code}")
                print(f"   å“åº”å†…å®¹: {response.text}")
                return False
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ çˆ¬è™«æœåŠ¡è¯·æ±‚å¤±è´¥: {e}")
            return False

    def test_literature_list(self, page=1, size=5):
        """æµ‹è¯•åŒ»ç–—æ–‡çŒ®åˆ—è¡¨æŸ¥è¯¢"""
        try:
            print(f"ğŸ“š æµ‹è¯•æ–‡çŒ®åˆ—è¡¨æŸ¥è¯¢ - é¡µç : {page}, å¤§å°: {size}")
            
            params = {
                "pageNum": page,
                "pageSize": size
            }
            
            response = self.session.get(
                f"{self.base_url}/medical-literature/list",
                params=params,
                timeout=15
            )
            
            if response.status_code == 200:
                data = response.json()
                literature_list = data.get('data', {}).get('list', [])
                total = data.get('data', {}).get('total', 0)
                
                print(f"âœ… æ–‡çŒ®åˆ—è¡¨æŸ¥è¯¢æˆåŠŸ!")
                print(f"   - æ€»æ•°é‡: {total}")
                print(f"   - å½“å‰é¡µæ•°é‡: {len(literature_list)}")
                
                if literature_list:
                    print("   - æœ€æ–°æ–‡çŒ®:")
                    for i, lit in enumerate(literature_list[:3], 1):
                        title = lit.get('title', 'æ— æ ‡é¢˜')[:50]
                        source = lit.get('crawlSource', 'æœªçŸ¥æ¥æº')
                        print(f"     {i}. {title}... (æ¥æº: {source})")
                
                return True
            else:
                print(f"âŒ æ–‡çŒ®åˆ—è¡¨æŸ¥è¯¢å¤±è´¥: HTTP {response.status_code}")
                return False
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ æ–‡çŒ®åˆ—è¡¨æŸ¥è¯¢è¯·æ±‚å¤±è´¥: {e}")
            return False

    def test_arxiv_direct(self, keyword="artificial intelligence", max_results=3):
        """ç›´æ¥æµ‹è¯•arXiv API"""
        try:
            print(f"ğŸ”¬ ç›´æ¥æµ‹è¯•arXiv API - å…³é”®è¯: {keyword}")
            
            # æ„å»ºarXivæŸ¥è¯¢URL
            query = f"all:{keyword}"
            url = f"http://export.arxiv.org/api/query?search_query={query}&start=0&max_results={max_results}"
            
            response = requests.get(url, timeout=15)
            
            if response.status_code == 200:
                print(f"âœ… arXiv APIæµ‹è¯•æˆåŠŸ!")
                print(f"   - å“åº”é•¿åº¦: {len(response.text)} å­—ç¬¦")
                
                # ç®€å•è§£æXMLå†…å®¹
                content = response.text
                if "<entry>" in content:
                    entry_count = content.count("<entry>")
                    print(f"   - æ‰¾åˆ°è®ºæ–‡æ¡ç›®: {entry_count} ç¯‡")
                else:
                    print("   - æœªæ‰¾åˆ°è®ºæ–‡æ¡ç›®")
                
                return True
            else:
                print(f"âŒ arXiv APIæµ‹è¯•å¤±è´¥: HTTP {response.status_code}")
                return False
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ arXiv APIè¯·æ±‚å¤±è´¥: {e}")
            return False

    def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("=" * 60)
        print("ğŸ¥ å¥åº·ä½“æ£€ç³»ç»Ÿ - çˆ¬è™«åŠŸèƒ½ç»¼åˆæµ‹è¯•")
        print("=" * 60)
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ç›®æ ‡æœåŠ¡: {self.base_url}")
        print()
        
        results = []
        
        # 1. æµ‹è¯•åç«¯å¥åº·çŠ¶æ€
        results.append(("åç«¯å¥åº·æ£€æŸ¥", self.test_backend_health()))
        print()
        
        # 2. æµ‹è¯•arXiv APIç›´è¿
        results.append(("arXiv APIç›´è¿", self.test_arxiv_direct()))
        print()
        
        # 3. æµ‹è¯•çˆ¬è™«æœåŠ¡
        test_keywords = ["äººå·¥æ™ºèƒ½", "machine learning", "åŒ»ç–—å¥åº·"]
        for keyword in test_keywords:
            result = self.test_crawler_service(keyword, 3)
            results.append((f"çˆ¬è™«æµ‹è¯•({keyword})", result))
            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            print()
        
        # 4. æµ‹è¯•æ–‡çŒ®åˆ—è¡¨æŸ¥è¯¢
        results.append(("æ–‡çŒ®åˆ—è¡¨æŸ¥è¯¢", self.test_literature_list()))
        print()
        
        # è¾“å‡ºæµ‹è¯•æ€»ç»“
        print("=" * 60)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
        print("=" * 60)
        
        success_count = 0
        for test_name, success in results:
            status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
            print(f"{test_name:<20} {status}")
            if success:
                success_count += 1
        
        print()
        print(f"æ€»æµ‹è¯•é¡¹: {len(results)}")
        print(f"é€šè¿‡é¡¹æ•°: {success_count}")
        print(f"æˆåŠŸç‡: {success_count/len(results)*100:.1f}%")
        
        if success_count == len(results):
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¬è™«ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼")
        elif success_count > len(results) // 2:
            print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œç³»ç»ŸåŸºæœ¬å¯ç”¨ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥é¡¹ã€‚")
        else:
            print("\nğŸš¨ å¤šé¡¹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®å’ŒæœåŠ¡çŠ¶æ€ã€‚")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¥åº·ä½“æ£€ç³»ç»Ÿçˆ¬è™«æµ‹è¯•å·¥å…·")
    parser.add_argument("--url", default="http://localhost:9090", 
                       help="åç«¯æœåŠ¡åœ°å€ (é»˜è®¤: http://localhost:9090)")
    parser.add_argument("--keyword", default="äººå·¥æ™ºèƒ½", 
                       help="æµ‹è¯•å…³é”®è¯ (é»˜è®¤: äººå·¥æ™ºèƒ½)")
    parser.add_argument("--count", type=int, default=5, 
                       help="çˆ¬å–æ•°é‡ (é»˜è®¤: 5)")
    parser.add_argument("--quick", action="store_true", 
                       help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    
    args = parser.parse_args()
    
    tester = CrawlerTestTool(args.url)
    
    if args.quick:
        # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
        print("ğŸš€ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
        tester.test_backend_health()
        tester.test_crawler_service(args.keyword, args.count)
    else:
        # ç»¼åˆæµ‹è¯•æ¨¡å¼
        tester.run_comprehensive_test()

if __name__ == "__main__":
    main()
